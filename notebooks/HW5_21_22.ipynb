{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW5_21_22.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Vzk_94Ey_KZz"},"source":["#**Deep Learning Homework 5: *Autoencoders***\n","### MSc Computer Science, Data Science, Cybersecurity Computer Engeneering @UniPD\n","### 2nd semester - 6 ECTS\n","### Prof. Nicol√≤ Navarin & Prof. Alessandro Sperduti\n","---\n","\n","\n","In this homework, we will deal with _dimensionality reduction_ and learn how to develop a simple _Autoencoder_.\n","In the first part, we will learn how to develop a simple shallow autoencoder, then we will develop a deep version. Next, we will experiment with the application of autoencoder on denoising data task (denoising-autoencoder). Finally, we will apply this model to sequential domains, considering the IMDB dataset seen in HW4."]},{"cell_type":"code","metadata":{"id":"-SUqAWqAXRr1"},"source":["# import the usual libriaries\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Dataset\n","We load the CIFAR-10 dataset, using `tf.keras.datasets`. The dataset contains 60,000 training images and 10,000 testing images.\n","The images are originally in RGB format, but we will convert them to grayscale for convenience. The value of each pixel is between 0 and 255, and it represents a point of an image of size 32 x 32. We will normalize all values between 0 and 1, and we will flatten the 32x32 images into vectors of size 1024.\n","Finally, since no validation set is defined, we split the test set in a validation set and a new test set."],"metadata":{"id":"MSTTdGdgcvWW"}},{"cell_type":"code","source":["cifar_10 = keras.datasets.cifar10\n","(X_train_full, _), (x_test, _) = cifar_10.load_data() # The dataset is already divede in test and training\n","\n","X_train_full, x_test = np.mean(X_train_full, -1), np.mean(x_test, -1)\n","\n","X_train_full = X_train_full.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","X_train_full = X_train_full.reshape((len(X_train_full), np.prod(X_train_full.shape[1:])))\n","x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n","\n","# We extract the first 10000 samples of the training set, to use them as the validation set\n","x_valid, x_train = X_train_full[:10000], X_train_full[10000:]"],"metadata":{"id":"0T-SFSr3_zJI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6KiNMUN9_ke"},"source":["## [TO COMPLETE] Exercise 5.1: Singular Value Decomposition\n","\n","Similar to Principal component analysis (PCA), Singular Value Decomposition (SVD) is a standard linear dimensionality reduction method. They both linealry combine the features of the original high-dimensional dataset and project them into a lower-dimensional space, ideally retaing most of thier intrinsic properties.\n","\n","In this first part of the HW, we will focus our attention on SVD decomposition and its performances. Given a matrix $X$, the SVD decomposes it into the product of two unitary matrices $V$ and $U$ and a rectangular diagonal matrix of singular values $S$:\n","\n","$$ X=V \\cdot S \\cdot U^T.$$\n","\n","The SVD is already implemented in NumPy as `np.linalg.svd`. In our case, the $X$ matrix will represent the training set, where each row is a sample (therefore the number of columns will be the number of input features). However, notice that the $X$ matrix has a huge number of rows (we have 50,000 input samples) and only 784 columns. If you are using the _Colab_ free plan, the quantity of available RAM may not be sufficient to compute the SVD of $X$. Therefore, to ease memory consumption and numerical stability, we resort to one property of the SVD and compute its equivalent version from the covariance matrix $C= X^T \\cdot X$, that can be decomposed as:\n","\n","$$ C= U \\cdot S^2 \\cdot U^T$$\n","\n","Since we need just the matrix $U$ to compute the compressed version of our data, this trick turns out to be a quick and good solution."]},{"cell_type":"code","metadata":{"id":"ckF8X9KDOQ_9"},"source":["def SVD(X, k):\n","  # Compute covariance matrix\n","  C = np.dot(X.T, X)\n","  # SVD decomposition\n","  U, s_sqr, U_T = np.linalg.svd(C, full_matrices=False)\n","  # Limit the number columns of U to k\n","  U_k=U[:,:k]\n","  return U_k"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pC6SJH7vzGYa"},"source":["Let's define the `ENCODING_DIM`, that will be the size of the compressed version of input data, and project the low-dimensional versions of the training set and the test set as well."]},{"cell_type":"code","metadata":{"id":"pIvHTxWpOzCX"},"source":["ENCODING_DIM = 120\n","\n","U_k = SVD(x_train, ENCODING_DIM)\n","\n","x_training_svd = np.dot(x_train, U_k)\n","x_test_svd = np.dot(x_test, U_k)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vt-gKtEwzvr0"},"source":["We now reconstruct back the original input and check how much information was lost due to the compression. We do so by computing the mean squared error between the original input and the reconstruction, and by plotting the reconstructed images."]},{"cell_type":"code","metadata":{"id":"3GpWcHRkPBJV"},"source":["x_training_reco = np.dot(x_training_svd, U_k.T)\n","x_test_reco = np.dot(x_test_svd, U_k.T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CPhgcMZPmtM"},"source":["accuracy_train = ((x_train - x_training_reco)**2).mean()\n","accuracy_test = ((x_test - x_test_reco)**2).mean()\n","\n","print(\"Training mse: %.5f\" % ( accuracy_train))\n","print(\"Test mse: %.5f\" % ( accuracy_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EuBw_l_Cgo1o"},"source":["def plot_img(n, images_sets: list, title=\"\"):\n","  plt.figure(figsize=(20, 4))\n","  for i in range(n):\n","    for set_idx, images in enumerate(images_sets):\n","      ax = plt.subplot(len(images_sets), n, i + 1 + set_idx*n)\n","      plt.imshow(images[i].reshape(32, 32))\n","      plt.gray()\n","      ax.get_xaxis().set_visible(False)\n","      ax.get_yaxis().set_visible(False)\n","  plt.suptitle(title)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CfqN6d8-37mu"},"source":["Let's see how well the input can be reconstructed by displaying a few of the input images and the corresponding reconstructions. Obviously, all these evaluations have to be done on the test set.\n","\n","The first row of images corresponds to input data, while the second one contains the reconstructions."]},{"cell_type":"code","metadata":{"id":"FICZjh2mgz_7"},"source":["plot_img(10, [x_test, x_test_reco], title = \"Original (top) vs Reconstructed (bottom)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**[TO COMPLETE]**: What happens by varying the `ENCODING_DIM`? Try it out and discuss the results in this cell.\n","\n","_Answer:_"],"metadata":{"id":"owwYD9VOVhsx"}},{"cell_type":"markdown","metadata":{"id":"O7crlDsMGrBN"},"source":["## [TO COMPLETE] Exercise 5.2: Shallow Linear Autoencoder\n","Let's define a model that consists of a single fully-connected neural layer. The hidden layer and the output layer act as the encoder and the decoder, respectively. Differently than the previous HW when we used `Keras.Sequential()`, here we define the two parts of the model (encoder and decoder) separately and then we create the final model. \n"]},{"cell_type":"code","metadata":{"id":"GfBJJLioaWJN"},"source":["ENCODING_DIM = 256\n","INPUT_DIM = x_train.shape[-1]\n","\n","input_img = tf.keras.layers.Input(shape=(INPUT_DIM,))\n","# Define the encoder...\n","encoded = tf.keras.layers.Dense(ENCODING_DIM, activation='linear')(input_img)\n","\n","# ...the decoder...\n","decoded = tf.keras.layers.Dense(INPUT_DIM, activation='linear')(encoded)\n","\n","# and the autoencoder\n","autoencoder = tf.keras.models.Model(input_img, decoded)\n","\n","# In order to visualize the learned encoding, define a model that computes the \n","# two parts separately.\n","encoder = tf.keras.models.Model(input_img, encoded)\n","encoded_input = tf.keras.layers.Input(shape=(ENCODING_DIM,))\n","decoder_layer = autoencoder.layers[-1]\n","decoder = tf.keras.models.Model(encoded_input, decoder_layer(encoded_input))\n","\n","# Finally, let's call the compile method\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='mse')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_4H-OFzI4tA"},"source":["Print the model summary."]},{"cell_type":"code","metadata":{"id":"4gr3B9qoC3zQ"},"source":["autoencoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cz3q7uSsKA2N"},"source":["Train the model."]},{"cell_type":"code","metadata":{"id":"Szm0L3I03-G9"},"source":["history = autoencoder.fit(x_train, x_train, epochs=15, batch_size=512, shuffle=True, validation_data=(x_valid, x_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfoaQXZgKJZm"},"source":["Plot the loss and the accuracy curves on the validation set, and the accuracy on the test set.\n"]},{"cell_type":"code","metadata":{"id":"15ZjvbTGDB8i"},"source":["def plot_loss(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['loss'])\n","  plt.plot(history.epoch,history.history['val_loss'])\n","  plt.grid()\n","  plt.title('loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Vb4ilH-DDyS"},"source":["plot_loss(history)\n","\n","scores = autoencoder.evaluate(x_test, x_test, verbose=2)\n","print(\"Test mse: %.5f\" % (scores))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgG5Yt9ddmq-"},"source":["encoded_imgs = encoder.predict(x_test)\n","decoded_imgs = decoder.predict(encoded_imgs)\n","\n","plot_img(10, [x_test, decoded_imgs], title = \"Original (top) vs Reconstructed (bottom)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**[TO COMPLETE]** Check the results and compare them versus the results obtained with the SVD. Give an explanation of the relation between the results obtained  by the shallow linear autoencoder and the ones obtained by the SVD decomposition.\n","\n","\n","_Answer:_"],"metadata":{"id":"mhm-2l4PXjbh"}},{"cell_type":"markdown","metadata":{"id":"BhhQcerS4Xm0"},"source":["##[TO COMPLETE] Exercise 5.3: Shallow Non-linear Autoencoders\n","\n","**[TO COMPLETE]** Replicate the code of Exercise 5.2 but in this case, instead of using linear activation functions use non-linear ones. Choose the most appropriate non-linear function, and motivate your choice. Then discuss the results in relation to those obtained in Exercise 5.2. \n","\n","Insert your code and theoretical discussion into cells immediately below this one.\n"]},{"cell_type":"markdown","metadata":{"id":"UjzlA8YV4ux5"},"source":["##[TO COMPLETE] Exercise 5.4: Deep Autoencoder\n","**[TO COMPLETE]** Build a deep version of the Autoencoder defined above. The autoencoder has to use at least 5 layers. The model will use $n$ layers for encoding, and $n-1$ for decoding. The layers sizes of the encoding part decrease at each layer (e.g., `INPUT_DIM->128->64`, where 64 is the encoding dim). The decoding part layers dimensions progression turns out to be mirrored (e.g., `128->INPUT_DIM`, the final overall structure recalls an hourglass ‚è≥).\n","As usual, print the model summary, the loss curves during the training, the achieved loss on the test set, and some input images with the corresponding decoding.\n"]},{"cell_type":"code","metadata":{"id":"S6O7Bq8C4udX"},"source":["INPUT_DIM = x_train.shape[-1]\n","\n","input_img = tf.keras.layers.Input(shape=(INPUT_DIM,))\n","#[TO COMPLETE]\n","autoencoder = tf.keras.models.Model() #[TO COMPLETE]\n","\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='mse')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XL8uh9MFV03"},"source":["autoencoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITsWoDrJ4uZy"},"source":["history = autoencoder.fit(x_train, x_train, epochs= **TO COMPLETE**, batch_size= **TO COMPLETE**, shuffle=True, validation_data=(x_valid, x_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3o00moO45Y8F"},"source":["plot_loss(history)\n","\n","scores = autoencoder.evaluate(x_test, x_test, verbose=2)\n","print(\"test mse: %.5f\" % (scores))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEmJTRwdFsle"},"source":["decoded_imgs = autoencoder.predict(x_test)\n","\n","plot_img(10, [x_test, decoded_imgs], title = \"Original (top) vs Reconstructed (bottom)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p55h0PXw5oI4"},"source":["## [TO COMPLETE] Exercise 5.5: Denoising Autoencoder\n","\n","Let's now use a shallow autoencoder to denoise the input data. The idea is that only meaningful information will be encoded, thus filtering out useless noise.\n","Firstly, define a noisy input by adding some gaussian noise to our input data. We define a noise factor that can be used to modify the amount of noise to add to the input data. Check how much it influences the denoising capability of the autoencoder."]},{"cell_type":"code","metadata":{"id":"UEgtUSt1531f"},"source":["noise_factor = 0.3\n","\n","x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n","x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n","x_valid_noisy = x_valid + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_valid.shape)\n","\n","# we still have to be sure that the color values are in the [0,1] range:\n","x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n","x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n","x_valid_noisy = np.clip(x_valid_noisy, 0., 1.)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjpSfN_ZQJge"},"source":["Plot some noisy inputs:"]},{"cell_type":"code","source":["plot_img(10, [x_test_noisy, x_test], title = \"Noisy (top) vs Original (bottom)\")"],"metadata":{"id":"pITmh1WzcOTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_i6Z8XfQ5iq"},"source":["**[TO COMPLETE]** Define a shallow autoencoder able to compute a de-noised version of the input (use a noise_factor $\\geq 0.3$). "]},{"cell_type":"code","metadata":{"id":"omRH5b-M6YA_"},"source":["INPUT_DIM = x_train.shape[-1]\n","\n","input_img = tf.keras.layers.Input(shape=(INPUT_DIM,))\n","#[TO COMPLETE]\n","autoencoder = tf.keras.models.Model(input_img, decoded)\n","\n","autoencoder.compile() #[TO COMPLETE]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgynPvZ4GUWh"},"source":["autoencoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PRx4JtF_RGQz"},"source":["**[TO COMPLETE]** Train the model by passing the noisy input and the clean target."]},{"cell_type":"code","metadata":{"id":"wMMPGQbO6lBY"},"source":["history = autoencoder.fit() #[TO COMPLETE]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCw0jn077Wn9"},"source":["plot_loss(history)\n","\n","scores = autoencoder.evaluate(x_test_noisy, x_test, verbose=2)\n","print(\"Test mse: %.5f\" % (scores))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aHOQfAPZRYUY"},"source":["**[TO COMPLETE]** Check the result by plotting some input images and the corresponding denoised outputs."]},{"cell_type":"code","metadata":{"id":"XUWwHDT3HiGX"},"source":["decoded_imgs = autoencoder.predict(x_test_noisy)\n","\n","plot_img(10, [x_test_noisy, x_test, decoded_imgs])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lInEGwkch5MW"},"source":["## Exercise 5.6: Linear Autoencoder for sequences\n","\n","Let's define a linear autoencoder for sequences. In this case we will use the IMDB dataset (already introduced in HW4). To have a model that can be trained and tested in a reasonable time (and that works also with the memory limitation that we have in _Colab_), we will limit the number of training samples and test samples."]},{"cell_type":"code","metadata":{"id":"nOJLUpNuaEc5"},"source":["num_words = 100\n","(X_train, _), (X_test, _) = keras.datasets.imdb.load_data(num_words=num_words)\n","\n","X_train=X_train[:10000]\n","\n","(X_valid, X_test) = X_test[:1250], X_test[-1250:]\n","\n","word_index = keras.datasets.imdb.get_word_index()\n","\n","reverse_index = {word_id + 3: word for word, word_id in word_index.items()}\n","reverse_index[0] = \"<pad>\" # padding\n","reverse_index[1] = \"<sos>\" # start of sequence\n","reverse_index[2] = \"<oov>\" # out-of-vocabulary\n","reverse_index[3] = \"<unk>\" # unknown\n","\n","def decode_review(word_ids):\n","    return \" \".join([reverse_index.get(word_id, \"<err>\") for word_id in word_ids])\n","\n","maxlen = 90\n","X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n","X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n","X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoXke75qeitM"},"source":["In this case, we want to use as input/target a one-hot representation for each word. To convert the index representation provided by IMDB dataset loader we use the to_categorical method to transform them in the corresponding one hot representation."]},{"cell_type":"code","metadata":{"id":"Fpa0BwSxaXiw"},"source":["from tensorflow.keras.utils import to_categorical\n","\n","\n","X_train_one_hot=to_categorical(X_train_trim)\n","X_test_one_hot=to_categorical(X_test_trim)\n","X_valid_one_hot=to_categorical(X_valid_trim)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gApoz_RFiciZ"},"source":["Define a linear shallow autoencoder for sequences. The structure will be similar to the model defined in Exercise 5.2, while the used encoding layer is defined by using `tf.keras.layers.SimpleRNN`. Note that it uses linear activations. The decoding layer exploits [tf.keras.layers.TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) that allows using the same dense cell at each time step of the sequence. "]},{"cell_type":"code","metadata":{"id":"S69HgNmAicUp"},"source":["inputs = tf.keras.layers.Input(shape=(maxlen, num_words))\n","encoded = tf.keras.layers.SimpleRNN(50, return_sequences=True, activation='linear')(inputs)\n","\n","decoded = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_words, activation='linear'))(encoded)\n","\n","sequence_autoencoder = tf.keras.models.Model(inputs, decoded)\n","encoder = tf.keras.models.Model(inputs, encoded)\n","\n","sequence_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=[\"CategoricalAccuracy\"])\n","\n","sequence_autoencoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"haFSyKBRjsWW"},"source":["history = sequence_autoencoder.fit(X_train_one_hot, X_train_one_hot, epochs=50, batch_size=128, shuffle=True, validation_data=(X_valid_one_hot, X_valid_one_hot))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UXgzgmSA-ZV-"},"source":["Let's plot the accuracy and the loss trends and check the reconstruction capability of the model by plotting the reconstruction of a test sample"]},{"cell_type":"code","metadata":{"id":"3N96nqx2nrt9"},"source":["def plot_categorical_accuracy(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['categorical_accuracy'])\n","  plt.plot(history.epoch,history.history['val_categorical_accuracy'])\n","  plt.title('accuracy')\n","\n","plot_loss(history)\n","\n","plot_categorical_accuracy(history)\n","\n","scores = sequence_autoencoder.evaluate(X_test_one_hot, X_test_one_hot, verbose=2)\n","print(\"%s: %.2f%%\" % (sequence_autoencoder.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNQmY1kynxe7"},"source":["decoded_text = sequence_autoencoder.predict(X_test_one_hot)\n","decode_index= np.argmax(decoded_text[500], axis=1)\n","input_text= np.argmax(X_test_one_hot[500], axis=1)\n","\n","print(decode_review(input_text))\n","print(decode_review(decode_index))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bW5BipraiQiE"},"source":["##[TO COMPLETE] Exercise 5.7: Non-Linear Autoencoder for sequences\n","**[TO COMPLETE]**: Replicate the code of the above exercise, but instead of using a simpleRNN with linear activation do the same  using  non-linear activation functions and using an LSTM layer. Choose the most appropriate non-linear function, and motivate your choice. Then discuss the results in relation to those obtained by the linear autoencoder for sequences.\n","\n","Hint: using a non-linear function also in the dense layer after the RNN/LSTM one will help to obtain better results. The choice of this function should be based on the type of output data."]},{"cell_type":"code","source":[""],"metadata":{"id":"l4j6OABZdCUo"},"execution_count":null,"outputs":[]}]}