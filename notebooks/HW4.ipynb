{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of HW4.ipynb","provenance":[{"file_id":"1MNAmuFSBNr63FWqlq3QzEE2Bgv4Bzt5C","timestamp":1652246465134},{"file_id":"1UP3aInTxt4avR7vt3_S6nwLMClA_qkRY","timestamp":1652183666439},{"file_id":"11CmjcOpjWeZSOOr7TEmV5HbFIYx2EZe3","timestamp":1588863979233},{"file_id":"1S2IocRQdDE09i5KR6ZCchdji6SEbVUTc","timestamp":1588836539424},{"file_id":"1gYhf63p6lyrvoXLQ6NipjpfNADWVKRvk","timestamp":1588636925965}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S4Iui5UetZvC"},"source":["#**Deep Learning Homework 4: Recurrent Neural Networks & Transformer**\n","\n","### MSc Computer Science, Data Science, Cybersecurity @UniPD\n","### 2nd semester - 6 ECTS\n","### Prof. Nicol√≤ Navarin & Prof. Alessandro Sperduti\n","---\n","In this homework, we will explore how to develop a simple Recurrent Neural Network (RNN) for sentiment analysis. We will use the IMDB dataset---it contains the text of some reviews and the sentiment given by their authors (either positive or negative). The input to the RNN is the sequence of words that compose a review, so the learning task consists in predicting the overall sentiment of the review.\n","In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like LSTM and GRU. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will solve the same classification problem with a Transformer, in order to have a closer look at its internal functioning.\n","\n","**NOTE**: in order to run this notebook without problems, please connect to a *GPU runtime*. You can do so by clicking on the RAM / Disk icon in the upper right part of the notebook, then on *Change runtime tipe* at the bottom of the page, and then select the GPU hardware accelerator.  "]},{"cell_type":"markdown","metadata":{"id":"WTXPA4gUKkpp"},"source":["##Exercise 4.1: Simple RNN\n","\n","Let's start by importing Tensorflow, Keras and Numpy"]},{"cell_type":"code","metadata":{"id":"G-hX557NDQ9s"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZxFjnG7Kuwi"},"source":["###Load dataset:\n","In this HW, we use the same datset used in the HW2, the IMDB dataset. The dataset contains 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes. For convenience, the words are indexed by the overall frequency in the dataset, so that for example the integer \"3\" encodes the 3rd most frequent word in the data. For testing purposes, we will only consider the first 10,000  most common words.\n","By default, the load_data method returns a breakdown of the dataset into training and test sets. Both these sets contain 25,000 samples. To also have a validation set, we split the test set in half."]},{"cell_type":"code","metadata":{"id":"NpaX6AGOD77D"},"source":["num_words = 10000\n","(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n","(X_valid, X_test) = X_test[:12500], X_test[12500:]\n","(y_valid, y_test) = y_test[:12500], y_test[12500:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZHxa95qQ0jb"},"source":["Let's also get the word indexs (word to word-id)"]},{"cell_type":"code","metadata":{"id":"LszZMh45X8Wz"},"source":["word_index = keras.datasets.imdb.get_word_index()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxLLVb5mRc0v"},"source":["Now we create a reverse index (word-id to word) method. Moreover, we add three special word-ids to encode:\n","- the padding;\n","- the start of a sequence;\n","- a word that is not in the vocabulary of the first 10,000 most common words.\n","\n","Moreover, we also add an \"unknown\" placeholder for all the other symbols (not words) that may occur. Notice that Keras does not use index 0, so we can shift the indices only by 3 positions."]},{"cell_type":"code","metadata":{"id":"NeZwDtDjYFt3"},"source":["reverse_index = {word_id + 3: word for word, word_id in word_index.items()}\n","reverse_index[0] = \"<pad>\" # padding\n","reverse_index[1] = \"<sos>\" # start of sequence\n","reverse_index[2] = \"<oov>\" # out-of-vocabulary\n","reverse_index[3] = \"<unk>\" # unknown\n","\n","def decode_review(word_ids):\n","    return \" \".join([reverse_index.get(word_id, \"<err>\") for word_id in word_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIMY115nRzXQ"},"source":["Let's print a training sample and its target value"]},{"cell_type":"code","metadata":{"id":"jnBRMiZJYQpm"},"source":["decode_review(X_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1tzUWldYT8X"},"source":["y_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0O7x_P0IqihK"},"source":["Because of a limit of Keras, to feed the input data into an RNN model we have to create sequences that have the same length. We use keras.preprocessing.sequence.pad_sequences() to preprocess X_train: this will create a 2D array of 25,000 rows (one per review) and maxlen=500 columns. Because of that, reviews longer than 500 words will be cut, while reviews shorter than 500 words will be padded with zeros."]},{"cell_type":"code","metadata":{"id":"wtMUMGMUYdAA"},"source":["maxlen = 500\n","X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n","X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n","X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jLxP0KW1104"},"source":["### Model Definition\n","Let's define the model: \n","- The first layer is an Embedding layer, with input_dim=num_words and output_dim=10. The model will gradually learn to represent each of the 10,000 words as a 10-dimensional vector. So the next layer will receive 3D batches of shape (batch size, 500, 10)\n","- The second layer is the recurrent one. In particular, in this case, we use a [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) \n","- The output layer \n","\n","**[TO COMPLETE]** In the following cell, we already inserted in the model the first Embedding layer. Add the recurrent layer (using 32 units), and the output layer. Select the right activation function for the output layer and motivate your choice. Finally, select the right loss function inserting the right value for the \"loss\" parameter in `model.compile()` and motivate your choice.\n"]},{"cell_type":"markdown","source":["Please, insert your answer in this text cell.\n","\n","**Answer:** [TO COMPLETE]"],"metadata":{"id":"RtnsfSkOs3r7"}},{"cell_type":"code","metadata":{"id":"xPnihHKiYf__"},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model.add([TO COMPLETE])\n","model.add([TO COMPLETE])\n","\n","model.compile(loss=[TO COMPLETE], optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IBNh_o2I2SN"},"source":["Let's print a summary of the model. Specifically, note the number of parameters of the RNN layer."]},{"cell_type":"code","metadata":{"id":"6AKJ1qsgYkH7"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTrvel3KRkYy"},"source":["Now we have to train the model:"]},{"cell_type":"code","metadata":{"id":"zCqSgX9WYmaR"},"source":["history = model.fit(X_train_trim, y_train,\n","                    epochs=10, batch_size=128, validation_data=(X_valid_trim, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Fvscg67R4H1"},"source":["Print the values of accuracy and the loss , and evaluate the model on the test set"]},{"cell_type":"code","metadata":{"id":"GKXRHieZbu-2"},"source":["def plot_loss(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch, history.history['loss'])\n","  plt.plot(history.epoch, history.history['val_loss'])\n","  plt.title('loss')\n","\n","def plot_accuracy(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch, history.history['accuracy'])\n","  plt.plot(history.epoch, history.history['val_accuracy'])\n","  plt.title('accuracy')\n","\n","plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHc_4yOASBXu"},"source":["## Exercise 4.2: LSTM and GRU\n","**[TO COMPLETE]**: In this Exercise, you have to implement 2 models, similar to the previous one that, instead of exploiting the RNN layer, use an LSTM and a GRU Layer respectively. For each model print the summary. Then, train it and plot the values of accuracy and loss. Finally, discuss the differences in terms of performance, the number of parameters, and training time. Note that you can use a different number of units than the one used in the RNN example."]},{"cell_type":"code","source":["# LSTM - complete with model definition, summary, fit and plots."],"metadata":{"id":"Uc-l436osebp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GRU - complete with model definition, summary, fit and plots."],"metadata":{"id":"l6Oaq6LfsqKa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**[TO COMPLETE]**: In order to perform a fair comparison (definition of fair: models have to use more or less the same number of parameters) between the given RNN model and the other 2 models (LSTM and GRU), how many units do they have to use?\n","\n","**Insert cells (code and text) with results and discussion immediately after this cell** \n","\n"],"metadata":{"id":"-cvLVMmysdYJ"}},{"cell_type":"markdown","metadata":{"id":"g2JZMtVATw5T"},"source":["### Bidirectional LSTM\n","In conclusion, let's also have a look at the performances of a bidirectional LSTM instead of a simple LSTM. In Keras, it is possible to define a bidirectional layer by using [tf.keras.layers.Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional). Note that this wrapper requires as argument a layer, in our case we use [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n"]},{"cell_type":"code","metadata":{"id":"svybeLNAedBf"},"source":["model_bidirectional = keras.models.Sequential()\n","model_bidirectional.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model_bidirectional.add(keras.layers.Bidirectional(keras.layers.LSTM(32)))\n","model_bidirectional.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n","\n","model_bidirectional.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model_bidirectional.summary()\n","\n","history = model_bidirectional.fit(X_train_trim, y_train,\n","                    epochs=5, batch_size=128, validation_data=(X_valid_trim, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qICZr3WpfNI8"},"source":["plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model_bidirectional.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model_bidirectional.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CIv9BmpdCxaa"},"source":["## Exercise 4.3: Transformer"]},{"cell_type":"markdown","metadata":{"id":"DzcjLF0E99yj"},"source":["Let's now use a [Transformer](https://arxiv.org/abs/1706.03762) to perform the same task considered in the previous exercise. To define the Transformer we will define a class that inherits from [tensorflow.keras.layers.Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). This class represents a layer, that is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the `call()` method, and a state (weight variables), both defined in the constructor. In the following cell, we will override these two methods in order to define the TransformerLayer. \n","\n","The structure of the transformer is defined as follows:\n","*   A multi-head attention layer, defined using the [tensorflow.keras.layers.MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention)\n","*   Dropout operation (*dropout_att*)\n","*   Layer Normalization (*layernorm_att*)\n","*   A feedforward Neural Network, defenid by using keras.Sequential, and Dense layer\n","*   Dropout operation (*dropout_fnn*)\n","*   Layer Normalization (*layernorm_fnn*) that has in input the summation of the attention layer output and the feedforward NN output\n","\n"]},{"cell_type":"code","metadata":{"id":"t6iDxAJtCxCa"},"source":["from tensorflow.keras import layers\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_att = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm_fnn = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout_att = layers.Dropout(rate)\n","        self.dropout_fnn = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout_att(attn_output, training=training)\n","        out1 = self.layernorm_att(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout_fnn(ffn_output, training=training)\n","        return self.layernorm_fnn(out1 + ffn_output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Due to memory constraints of Transformers the maxlen is reduced to 200."],"metadata":{"id":"o-_SJbJUzHRV"}},{"cell_type":"code","source":["maxlen = 200\n","X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n","X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n","X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)"],"metadata":{"id":"aqylIExczGy5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDekYZOtFKei"},"source":["To manage the sequential structure of the input, we need to create an embedding of the word + a positional embedding. To perform this operation, similarly to what we did for defining the Transformer layer, we define a new class that inherits from *layer.Layers*. In the unbatched (i.e. single-input) version, this class has in input the index of a word and computes 2 embeddings: the embedding of the word, and the positional embedding. Finally, the method returns the summation of these two embeddings."]},{"cell_type":"code","metadata":{"id":"w3W8teD2GRPD"},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3SaW5NuGvAB"},"source":["Now we can define the Transformer model. The model is defined as follows:\n","\n","*   the Input layer\n","*   the TokenAndPositionEmbedding layer\n","*   the Transformer layer\n","*   2 Dense layers, the second one is the output layer.\n","\n","\n","**[TO COMPLETE]** Experiment with the `embed_dim, num_heads, ff_dim` and discuss the influence of these parameters in the obtained results, considering the accuracy of the output at the time required to perform the training phase. Report the discussion in the cell at the end of the notebook."]},{"cell_type":"code","metadata":{"id":"r2mB_q2AGYUH"},"source":["embed_dim = [TO COMPLETE]  # Embedding size for each token\n","num_heads = [TO COMPLETE]  # Number of attention heads\n","ff_dim = [TO COMPLETE]  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,))\n","embedding_layer = TokenAndPositionEmbedding(maxlen, num_words, embed_dim)\n","x = embedding_layer(inputs)\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","\n","model = keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    X_train_trim, y_train, batch_size=128, epochs=5, validation_data=(X_valid_trim, y_valid)\n",")"],"metadata":{"id":"WYP531vWd9qH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF3Bnc3fnFEV"},"source":["def plot_loss(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['loss'])\n","  plt.plot(history.epoch,history.history['val_loss'])\n","  plt.title('loss')\n","\n","def plot_accuracy(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['accuracy'])\n","  plt.plot(history.epoch,history.history['val_accuracy'])\n","  plt.title('accuracy')\n","\n","plot_loss(history)\n","\n","plot_accuracy(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Insert the discussion here**: [TO COMPLETE]"],"metadata":{"id":"eWIDKexKyJhD"}}]}