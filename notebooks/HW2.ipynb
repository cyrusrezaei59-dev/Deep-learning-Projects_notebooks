{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["#**Deep Learning Homework 2: *Optimize and Train Deep Models***\n","### MSc Computer Science, Data Science, Cybersecurity @UniPD\n","### 2nd semester - 6 ECTS\n","### Prof. Nicol√≤ Navarin & Prof. Alessandro Sperduti\n","---"],"metadata":{"id":"raUKrVPW0SO1"}},{"cell_type":"markdown","metadata":{"id":"QUc9frYnVBI_"},"source":["In this homework, we will explore how to develop a simple Deep Neural Network for a classification problem. We will explore two common libraries: TensorFlow and Keras.\n","Then we will explore how to face a well known problem that is common to encounter during the training phase: the Overfitting.\n","Finally, we will study how to perform a fair model selection.\n","Hint: Before starting the exercise take a look at how Tensorflow and Keras are designed.  https://keras.io/"]},{"cell_type":"markdown","metadata":{"id":"e-Nc7mFSVEiU"},"source":["##Exercise 2.1: Text Classficiation with Tensorflow and Keras\n","\n","In this first exercise we will develop a 3 layers Neural Network to perfrom classification.\n","\n","Let's start importing the libraries we will need and setting a couple of environmental variables.\n"]},{"cell_type":"code","metadata":{"id":"bWn9FzneVqJo"},"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import sklearn\n","import sys\n","import tensorflow as tf\n","from tensorflow import keras  # tf.keras\n","import tensorflow_datasets as tfds\n","import time\n","\n","import logging\n","logging.disable(logging.WARNING)\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n","\n","tf.random.set_seed(42)\n","# os.environ['TF_DETERMINISTIC_OPS']='1'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LT2HMYDEXcx0"},"source":["### Load Data: AG News Subset\n","\n","In this HW, we use the AG News Subset that is available in Tensorflow Dataset. The AG's news topic classification dataset is constructed by choosing the 4 largest topic classes from a larger news corups. The total number of training samples is 120,000 and testing 7,600. Each class contains 30,000 training samples and 1,900 testing samples. Each sample contains both the title and an excerpt of the article, but in this exercise we will use only the excerpt as input.\n","\n","We will split the training set into two 90%-10% splits in order to have a validation set."]},{"cell_type":"code","source":["### NOTE ### \n","# running this cell might yield download-related errors.\n","# just repeating the execution a couple of times should solve the error.\n","\n","train_ds, valid_ds, test_ds = tfds.load('ag_news_subset',\n","                                        as_supervised=True, # return a set of (text, label) tuples \n","                                        split=['train[:90%]', 'train[-10%:]', 'test'],\n","                                        batch_size=-1)  \n","                                        # ^ using batch_size=-1 returns Tensors instead of Dataset objects"],"metadata":{"id":"d8xZASzs26zD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tensors have two elements, data and labels; let's split them\n","# so we can transform the data\n","X_train, y_train = train_ds\n","X_valid, y_valid = valid_ds\n","X_test, y_test = test_ds"],"metadata":{"id":"BYkXCTv3NrvC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The AG News Subset dataset consists in news articles, each one composed by a sequence of words. In order to encode each article in a single tensor with a fixed number of elements, we will use multi-hot encoding. For testing purposes, we will only consider the first 10,000  most common words. Multi-hot-encoding our lists means turning them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. The obtained input representation indicates which words are present (at least one time) in the sentence."],"metadata":{"id":"8prUBtqq8JW7"}},{"cell_type":"code","source":["num_words = 1000\n","vectorize_layer = tf.keras.layers.TextVectorization(\n"," max_tokens=num_words,\n"," standardize=\"lower_and_strip_punctuation\",\n"," split=\"whitespace\",\n"," output_mode='multi_hot',\n"," pad_to_max_tokens=True)\n","\n","vectorize_layer.adapt(X_train)"],"metadata":{"id":"Brt087tDIDpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = vectorize_layer(X_train)\n","X_valid = vectorize_layer(X_valid)\n","X_test  = vectorize_layer(X_test)"],"metadata":{"id":"seuFLH2lNiDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGUg8TexZMNf"},"source":["### Define the model:\n","\n","Let's build a Sequential model (`keras.models.Sequential`) and add four layers to it by calling its `add()` method:\n","\n","\n","*   a input layer (`tf.keras.Input`) that is the first layer in your model. With this layer you should specify the `input_shape` argument, leaving out the batch size: `(num_words,)`;\n","*  a Dense layer (`keras.layers.Dense`) with 16 neurons, and the \"relu\" activation function;\n","* another Dense layer with 16 neurons, also with the \"relu\" activation function;\n","* a final Dense layer with 4 output neurons, and with the \"softmax\" activation (since we considering a classification task with four classes)."]},{"cell_type":"code","metadata":{"id":"OaoEISyaZL56"},"source":["model = keras.models.Sequential()\n","model.add(tf.keras.Input(shape=(num_words,)))\n","model.add(keras.layers.Dense(16, activation=\"relu\"))\n","model.add(keras.layers.Dense(16, activation=\"relu\"))\n","model.add(keras.layers.Dense(4, activation=\"softmax\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYfxUOQVZljh"},"source":["Call the model's `summary()` method to check if the model has been built correctly. Also, try using `keras.utils.plot_model()` to save an image of your model's architecture."]},{"cell_type":"code","metadata":{"id":"IpGkQl5dZoWq"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1NbkrXRZwOA"},"source":["keras.utils.plot_model(model, \"my_model.png\", show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mc3xzuA-33WH"},"source":["As you can see, the first dimension of each layer is not defined (`None`), indeed this dimension in each layer is variable and depends on the batch size."]},{"cell_type":"markdown","metadata":{"id":"6FAjFixuHSA_"},"source":["Instaed of using `add()` method it is also possibile to define the model using the following syntax"]},{"cell_type":"code","metadata":{"id":"yOhr57M9GPHy"},"source":["model = keras.models.Sequential([\n","    tf.keras.Input(shape=(num_words,)),\n","    keras.layers.Dense(16, activation=\"relu\"),\n","    keras.layers.Dense(16, activation=\"relu\"),\n","    keras.layers.Dense(4, activation=\"softmax\")\n","])\n","model.summary()\n","keras.utils.plot_model(model, \"my_model.png\", show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ySdJmzxaSuz"},"source":["### Optimize the model:\n","\n","After the model has been created, you must call its `compile()` method to specify the loss function and the optimizer to use.\n","\n","Since the model performs a classification task, and we have labels encoded as integers, we use the sparse categorical cross-entropy loss. As optimizer we use Adam.\n","\n","Moreover, you can optionally specify a list of additional metrics that should be measured during training. In this case we specify `metrics=[\"accuracy\"]`."]},{"cell_type":"code","metadata":{"id":"x3bbsY3uaeh5"},"source":["model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","              metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NB9SV0MpaeTg"},"source":["Now our model is ready to be trained. Call its `fit()` method, passing to it the input features (`X_train`) and the target classes (`y_train`). Set the number of epochs to 20. \n","In order to validate our model we will also pass the validation data by setting `validation_data=(X_valid, y_valid)`. Keras will compute the loss and the additional metrics (the accuracy in this case) on the validation set at the end of each epoch. If the loss on the training set is much lower than the one on the validation set, your model is probably overfitting the training set. Note: the `fit()` method will return a `History` object containing training stats."]},{"cell_type":"code","metadata":{"id":"5whx-LU3aoKn"},"source":["history = model.fit(X_train, y_train, epochs=20,\n","                    validation_data=(X_valid, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPxiw8iCADr-"},"source":["Let's plot the loss and the accuracy trends on both training and validation sets. We define the `plot_learning_acc_and_loss` function because we will reuse it in the next steps."]},{"cell_type":"code","metadata":{"id":"AJg-F7mUa7st"},"source":["def plot_learning_acc_and_loss(history):\n","    pd.DataFrame(history.history).plot(figsize=(8, 5))\n","    plt.grid(True)\n","    plt.show()\n","plot_learning_acc_and_loss(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9O3-sLmxansJ"},"source":["### [TO COMPLETE] Evaluate the model:\n","\n","Now, our model has been optimized on the training set, and as you can see the performance on the validation set in quite similar (so it does not overfit the training data). Let's now evaluate the performance of our model using the test set."]},{"cell_type":"code","metadata":{"id":"q-ZvJEcObdyB"},"source":["model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTSxlnwrII-n"},"source":["[TO COMPLETE] Explain why it is important to use test and validation, and why it is important to evaluate the model on the test set instead of the validation set. Finally, explain what is the usefulness of the validation set. Insert the discussion in this cell."]},{"cell_type":"markdown","metadata":{"id":"aeGlzKe864Gq"},"source":["## [TO COMPLETE] Exercise 2.2: Overfiting\n","\n","A common problem that occurs when you train a deep neural network is overfittig. Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data. In other words, the model learned patterns specific to the training data, which are irrelevant in other data.\n","As we have seen in the previous exercise, our model does not overfit the training data. In this exercise, we try to modify the training parameters in order to have a model that overfits.\n","Overfitting can have many causes and usually is a combination of some of them, for instance: too many parameters/ layers, too few training samples, wrong learning rate (usualy too high), etc..\n","\n","[TO COMPLETE] In the next cell define a new model (similar to the previuos one) that overfits the training data; then plot the trend of the loss in training and validation set."]},{"cell_type":"code","metadata":{"id":"T2B2DFEC8f88"},"source":["model = keras.models.Sequential()\n","#[TO COMPLETE] modify the previuos model in order to obtain a new model that overfits the training data\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=[TO COMPLETE]),#[TO COMPLETE] learning rate\n","              metrics=[\"accuracy\"])\n","\n","history = model.fit(X_train, y_train, epochs=[TO COMPLETE],#[TO COMPLETE] epochs\n","                    validation_data=(X_valid, y_valid))\n","\n","print(\"Model Evaluation\")\n","model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MS0fujEqh_m3"},"source":["Define the `plot_learning_curves` function that plots only the losses (not the accuracy)."]},{"cell_type":"code","metadata":{"id":"q5EsklC0DZYH"},"source":["def plot_learning_curves(history):\n","    plt.figure(figsize=(8, 5))\n","    plt.plot(history.epoch,history.history['loss'], label='train loss')\n","    plt.plot(history.epoch,history.history['val_loss'], label='valid loss')\n","    plt.legend()\n","    plt.title('loss')\n","    plt.grid(True)\n","    plt.show()\n","plot_learning_curves(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OW4Kf4fgDfOs"},"source":["### [TO COMPLETE] L1 norm\n","\n","One possible way to solve the overitting issue is by using regularization methods. The two most common regularization methods in Deep Learning are the L1-norm regularization and the L2-norm regularization. Both These techniques are based on limiting the capacity of models, by adding a parameter norm penalty to the objective function $\\mathcal{J}$:\n","$$\n","\\hat{\\mathcal{J}}(\\theta,\\mathbf{X},\\mathbf{y}) = \\mathcal{J}(\\theta,\\mathbf{X},\\mathbf{y}) + \\alpha \\Omega(\\theta)\n","$$\n","where $\\alpha$ is a hyperparameter that weighs the relative contribution of the norm penalty $\\Omega$.\n","Lets start by considering the L1-norm regularization where the regularization term is defined as:\n","$$\n"," \\Omega(\\theta)=||\\mathbf{W}||_1=\\sum_i |\\mathbf{w}|\n","$$\n","Let's find the values for the $\\alpha$ parameters that allow to remove the overfitting effect."]},{"cell_type":"code","metadata":{"id":"QzD8520dDgw3"},"source":["#In Keras is neccesary to add the regularizer by using the attribute kernel_regularizer\n","#to each layer whose weights will be considered in the Omega function.\n","#is it also possibile to consider the bias by using the attribute bias_regularizer \n","#tf.keras.regularizers.l1(alpha) perfroms the L1-norm regularization\n","\n","#[TO COMPLETE] copy the model that you previously defined that overfits the \n","#training data, and add the L1-norm regularization. Use a proper value for the\n","#alpha hyper-parameter, that prevents the model from overfitting. The results  \n","#reached by the model in terms of loss and accuracy (in all data sets) should be\n","#comparable with the ones obtained by the initial model defined in Exercise 2.1 \n","\n","#In general, a layer that exploits L1-norm regularization is defined as follows:\n","\n","#model.add(keras.layers.Dense(units=[TO COMPLETE], activation=[TO COMPLETE], kernel_regularizer=tf.keras.regularizers.l1([TO COMPLETE]))) \n","\n","model = keras.models.Sequential()\n","#[TO COMPLETE]\n","\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=[TO COMPLETE]),#[TO COMPLETE] learning rate\n","              metrics=[\"accuracy\"])\n","\n","history = model.fit(X_train, y_train, epochs=[TO COMPLETE],#[TO COMPLETE] epochs\n","                    validation_data=(X_valid, y_valid))\n","\n","print(\"Model Evaluation\")\n","model.evaluate(X_test, y_test)\n","plot_learning_curves(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CgnCGYUvDqbC"},"source":["### [TO COMPLETE] L2-norm\n","\n","L2-norm regularization is also known as weight decay. This strategy drives the weights closer to the origin by adding the regularization term omega which is defined as:\n","$$\n"," \\Omega(\\theta)=\\frac{1}{2}||\\mathbf{W}||_2^2\n","$$\n","\n","Let's find the values for the $\\alpha$ parameters that allow removing remove the overfitting effect with L2-norm."]},{"cell_type":"code","metadata":{"id":"qqNhu350DtJo"},"source":["#[TO COMPLETE] insert the model that you previously defined that overfit the \n","#trainig data, and add the L2-norm regularization. Use proper values for the\n","#alpha hyper-parameters, that prevents the model from overfitting. The results  \n","#reached by the model in terms of loss and accuracy (in all data sets) should be\n","#comparable with the ones obtained by the initial model defined in Exercise 2.1\n","\n","#In general, a layer that exploits L2-norm regularization is defined as follow:\n","\n","#model.add(keras.layers.Dense(units=[TO COMPLETE], activation=[TO COMPLETE],kernel_regularizer=tf.keras.regularizers.l2([TO COMPLETE]))) \n","\n","model = keras.models.Sequential()\n","#[TO COMPLETE]\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=[TO COMPLETE]),#[TO COMPLETE] learning rate\n","              metrics=[\"accuracy\"])\n","\n","history = model.fit(X_train, y_train, epochs=[TO COMPLETE],#[TO COMPLETE] epochs\n","                    validation_data=(X_valid, y_valid))\n","\n","print(\"Model Evaluation\")\n","model.evaluate(X_test, y_test)\n","plot_learning_curves(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvyKPb4DD25G"},"source":["###[TO COMPLETE] Early stopping\n","\n","Early Stopping is a form of regularization used to avoid overfitting. It is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. In order to evaluate the generalization error, early stopping requires that a validation dataset is evaluated during training. Then, when the validation error does not improve for a specific number of epochs (`patience` hyper-parameter), it stops the training phase."]},{"cell_type":"code","metadata":{"id":"MqNOOZ11D7cq"},"source":["#[TO COMPLETE] insert the model that you previously defined that overfit the \n","#trainnig data\n","model = keras.models.Sequential()\n","\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=[TO COMPLETE]),#[TO COMPLETE] learning rate\n","\n","              metrics=[\"accuracy\"])\n","\n","#Let's define a log dir in order to save the checkpoint file\n","logdir = os.path.join(os.curdir, \"my_logs\", \"run_{}\".format(time.time()))\n","\n","#In Keras the early stopping is manage by using the callbacks argument.\n","callbacks = [\n","    keras.callbacks.TensorBoard(logdir),\n","    keras.callbacks.EarlyStopping(patience=[TO COMPLETE]),#[TO COMPLETE] play with patience and find the value that prevents the model from overfitting.\n","\n","\n","    #Saving the checkpoints file allows to load the \"best\" model when the Early \n","    #stopping detect that the generalization error degrade (after 'patience' epochs)\n","    keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True),\n","]\n","\n","history = model.fit(X_train, y_train, epochs=[TO COMPLETE],#[TO COMPLETE] epochs\n","                    validation_data=(X_valid, y_valid),\n","                    callbacks=callbacks)\n","\n","print(\"Model Evaluation\")\n","#The early stopping stopped training after few epochs without progress, so your\n","#model may already have started to overfit the training set. Since the \n","#ModelCheckpoint callback only saved the best models (on the validation set) the\n","#last saved model is the best on the validation set.\n","model = keras.models.load_model(\"my_mnist_model.h5\")\n","model.evaluate(X_test, y_test)\n","plot_learning_curves(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCV8pwVzl9GK"},"source":["\n","## Exercise 2.3: Model Selection\n","\n","Hyperparameters are the parameters of the learning method itself which we have to specify a priori, i.e., before model fitting. In contrast, model parameters are parameters which arise as a result of the fit (the network weights). The aim of model selection is selecting the best hyperparameters for our deep network. Finding the right hyperparameters for a model can be crucial for the model performance on given data. For istance lets consider our model trained by using different values for the learning rate: "]},{"cell_type":"code","metadata":{"id":"Fw8Hs-AEmLKh"},"source":["learning_rates = [1e-4, 1e-2, 1e-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mym4GOgUmWqD"},"source":["histories = []\n","for learning_rate in learning_rates:\n","    model = keras.models.Sequential([\n","                                    tf.keras.Input(shape=(num_words,)),\n","                                    keras.layers.Dense(16, activation=\"relu\"),\n","                                    keras.layers.Dense(16, activation=\"relu\"),\n","                                    keras.layers.Dense(4, activation=\"softmax\")\n","                                     ])\n","    model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","              metrics=[\"accuracy\"])\n","              \n","    callbacks = [keras.callbacks.EarlyStopping(patience=2)]\n","\n","    history = model.fit(X_train, y_train,\n","                        validation_data=(X_valid, y_valid), epochs=10,\n","                        callbacks=callbacks)\n","    histories.append(history)\n","    print(\"evaluation on test set\")\n","    model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzTSjfwptkHJ"},"source":["Let's plot the results"]},{"cell_type":"code","metadata":{"id":"cIlN1vlioIYh"},"source":["for learning_rate, history in zip(learning_rates, histories):\n","    print(\"Learning rate:\", learning_rate)\n","    plot_learning_acc_and_loss(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JntujSWq3e5"},"source":["### [TO COMPLETE] GRID Search:\n","\n","Since a deep net has many hyperparameters, in order to find the best ones, we have to consider all the possible combinations of all of the possible values. One common method to perform this complex task is Grid-Search.\n","Given a set of values for each hyper-parameter, Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination. Finally, the model that obtained the best result on the validation set will be select.\n","\n","In order to perfrom Grid-Search we will use the `GridSearchCV` class from `scikit-learn`."]},{"cell_type":"markdown","metadata":{"id":"CPz629YSrDhN"},"source":["Let's Create a `build_model()` function that takes two arguments, `n_neurons` and `learning_rate`, and builds, compiles and returns a model with the given number of of neurons and the given learning rate.\n","In order to limit the time requirements of the process we will consider only these two hyper-paramters.\n","\n","[TO COMPLETE] in the following code cell define the `build_model` function."]},{"cell_type":"code","metadata":{"id":"oGSKWTTtqxZ4"},"source":["def build_model(n_units=30, learning_rate=1e-3):\n","    # The function has to build a model similar to the ones we used in previuous execises:\n","    #- a input layer \n","    #- one or two Dense layers composed of n_units and that exploit Relu activation function \n","    #- the output layer that uses the softmax activation function\n","    # Then, the model has to be compiled.\n","    model = keras.models.Sequential([\n","                                    #[TO COMPLETE]\n","                                     ])\n","    \n","    # [TO COMPLETE] Then, the model has to be compiled.\n","    \n","    return model\n","\n","    \n","#Create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model \n","#function to the constructor. This gives you a Scikit-Learn compatible predictor\n","keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PS6W3A2LxDg5"},"source":["Let's define the lists of hyper-parameters' values. Also in this case, we use a very limited size lists, but in a real-world scenario a reasonable amount of possible values should be considered (try to add some values and check how much the time required to perform the Grid-Search increases)"]},{"cell_type":"code","metadata":{"id":"y-uldCW3ru24"},"source":["param_distribs = {\n","    \"n_units\": [TO COMPLETE],# [TO COMPLETE] insert a list that contains few (2 or 3) reasonable value\n","    \"learning_rate\": [TO COMPLETE] #[TO COMPLETE] insert a list that contains few (2 or 3) reasonable values\n","    #Check how the time required to perform GRID search increases when increasing the number of values for each hyper-parameter.\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bEiBxVYtx_sK"},"source":["Use a `sklearn.model_selection.GridSearchCV` to search the hyperparameter space of your `KerasRegressor`"]},{"cell_type":"code","metadata":{"id":"iobF2Bvdr9ER"},"source":["from sklearn.model_selection import GridSearchCV\n","\n","grid_search = GridSearchCV(keras_reg, param_distribs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVGMcZz_yKSX"},"source":["Run the Grid-Search"]},{"cell_type":"code","metadata":{"id":"-BA3vgTjr-hw"},"source":["\n","grid_search.fit(X_train.numpy(), y_train.numpy(), epochs=5,#The number of epochs can be modified (check what happens by increasing it)\n","                validation_data=(X_valid.numpy(), y_valid.numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aadmkiOoyM94"},"source":["Print the best hyper-parameters, and evealuate the best model on the test set."]},{"cell_type":"code","metadata":{"id":"b0nL5OZkylNX"},"source":["print(grid_search.best_params_)\n","\n","model = grid_search.best_estimator_.model\n","model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]}]}