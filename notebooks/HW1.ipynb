{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW1.ipynb","provenance":[{"file_id":"1zNrLTlXEIbbG04s-PjRRXPXQbUOsV7IK","timestamp":1585583719095}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["#**Deep Learning Homework 1: *from the Perceptron to DNN***\n","### MSc Computer Science, Data Science, Cybersecurity @UniPD\n","### 2nd semester - 6 ECTS\n","### Prof. Nicolò Navarin & Prof. Alessandro Sperduti\n","---"],"metadata":{"id":"SY5WztYNneGg"}},{"cell_type":"markdown","metadata":{"id":"d8MTQQX3WaFa"},"source":["In this first homework, we are going to write our own simple feedforward neural network using `Python` and `NumPy` (the standard numeric library for Python). We will start by implementing just a simple neuron, or perceptron, then we define the training algorithm for this simple model.\n","The second part consists in defining a simple neural network to perform digits classification."]},{"cell_type":"markdown","metadata":{"id":"-bEFm73cYFQy"},"source":["## Exercise 1.1: Perceptron\n","\n","In this first exercise, we will implement a simple neuron, or perceptron, as described below. We will have just three inputs and one output neuron (we omit the bias term for now).\n","Notice how the perceptron simply performs a sum of the individual inputs multiplied by the corresponding weights mapped through an activation function $f(\\cdot)$.  This can also be expressed as a dot product of the weight vector $\\textbf{W}$ and the input vector $\\textbf{x}$, thus: $$\\hat{y}=f(\\textbf{W}^T \\textbf{x})$$"]},{"cell_type":"markdown","metadata":{"id":"hDlidWmiYuKB"},"source":["We will begin by implementing the perpetron by using the [numpy](https://docs.scipy.org/doc/numpy/reference/) library:"]},{"cell_type":"code","metadata":{"id":"i0UCur_TYckH"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQONq1k6Y1Zx"},"source":["### Training data\n","\n","Let's consider a very simple dataset. The dataset is made of four input vectors $\\textbf{x} \\in \\mathbb{R}^3$ and the corresponding desired target values $y$. In the table below, each row is a single sample; the first three columns are the input vector components, whereas the last column is the target output.\n","\n","||Input|     |    Output|\n","|:----:|:---:|:---:|---:|\n","| 1    | 1   | 0   | 0  |\n","| 1    | 0   | 0   | 0  |\n","| 0    | 1   | 0   | 1  |\n","| 0    | 0   | 0   | 1  |\n","\n","Notice that our target outputs are equal to the opposite of the first component of the input, therefore the task that the model should learn is very simple. We will see how the perceptron is able to learn that starting from this toy dataset.\n","\n","Now let's define the `X` and `y` matrices:"]},{"cell_type":"code","metadata":{"id":"u1FzAUxhY9PA"},"source":["# Our input data is a matrix, each row is one input sample\n","X = np.array([[1,1,0],\n","              [1,0,0],\n","              [0,1,0],\n","              [0,0,0]])\n","    \n","# The target output is a column vector in 2-D array format (.T means transpose)\n","y = np.array([[0,0,1,1]]).T\n","\n","print('X =',X)\n","print('y =',y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-W8wMnoZH71"},"source":["### Activation function\n","\n","As we said before, in order to define a perceptron we need to define the activation function $f(\\cdot)$. There are many possibile activation function that can be used, let's plot some of the most common ones:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Fi5ZK9zKZssu"},"source":["import matplotlib.pyplot as plt \n","\n","x = np.arange(-4,4,.01)\n","plt.figure()\n","plt.plot(x, np.maximum(x,0), label='ReLu')\n","plt.plot(x, 1/(1+np.exp(-x)), label='Sigmoid')\n","plt.plot(x, np.tanh(x), label='tanh')\n","plt.axis([-4, 4, -1.1, 1.1])\n","plt.title('Some Activation Functions')\n","plt.grid(True)\n","l = plt.legend()\n","plt.show()\n","\n","# Delete temporary variables, so not to cause any confusion later :-)\n","del x, l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdmJ6-qaZjuZ"},"source":["In this particular exercise we will use the sigmoid function. So let's define $f(\\cdot)$ as the sigmoid function\n","\n","$$f(x)=\\sigma(x)=\\frac{1}{1+\\exp^{-x}}$$"]},{"cell_type":"code","metadata":{"id":"wwnk5RgKZRox"},"source":["def f(x):\n","    # Sigmoid function\n","    return 1 / ( 1 + np.exp(-x) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vX-62n3Io0XU"},"source":["### Weight initialization\n","\n","Now we have to initialise the weights. Let's initialize them randomly, so that their mean is zero. The weights matrix maps the input space into the output space, therefore in our case $\\mathbf{W} \\in \\mathbb{R}^{3 \\times 1}$"]},{"cell_type":"code","metadata":{"id":"9f83YkaNtd5h"},"source":["# fix random seed for reproducibility\n","np.random.seed([42])\n","\n","# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n","W = 2 * np.random.random((3,1)) - 1\n","\n","print('W =', W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZd-aQxAuX49"},"source":["### Forward propagation\n","\n","Next, let's try to implement one round of forward propagation.  This means taking an input sample and moving it forward through the network, calculating the output of the network eventually.\n","\n","For our single neuron this is simply $\\hat{\\mathbf{y}} = f(\\mathbf{W}^T \\mathbf{x})$, where $\\mathbf{x}$ is one input vector.\n","\n","each input sample is arranged as a row of the matrix `X`, therefore we can access the first row by `X[0]`. Let's store it in the variable `X0` for easier access. We'll use `reshape` to make sure it's expressed as a column vector."]},{"cell_type":"code","metadata":{"id":"9gTx4QxUumvQ"},"source":["X0 = np.reshape(X[0], (3,1))\n","print(X0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpDCnmxWurjC"},"source":["The output $\\hat{y}$ for the first input can be calculated according to the formula given above"]},{"cell_type":"code","metadata":{"id":"idDZfTVKuyPf"},"source":["y_out = f(np.dot(W.T, X0))\n","\n","print('y_out =', y_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oda3Y3btu8Uz"},"source":["the target result is stored in `y[0]`.  If you check back, you can see we defined it to be $y_0=0$. You can see that our network is pretty far away from the right answer... this is why we need to backpropagate the error, to adjust the weights in the right direction!"]},{"cell_type":"markdown","metadata":{"id":"oGvgZCDFu-EM"},"source":["### Backpropagation\n","\n","The following step is updating the weights by propagating the error backwards in the network.  How this is done depends on the activation function, and namely on its derivative. The activation function of the considered model is the sigmoid, and its derivative is:\n","\n","$$\\sigma(x)'=\\sigma(x) \\cdot (1-\\sigma(x))$$\n","\n","Recall that the weight update is given as $\\Delta w_{ji} = -\\epsilon \\delta_j x_i$.\n","Our network has only one layer, so $x_i$ is just the input $\\mathbf{x}$, and a single output neuron so there is no actual need for index $j$. \n","\n","In matrix form we can calculate this for all the weights:\n","\n","$$\\Delta \\textbf{W} = -\\epsilon \\delta \\textbf{x}_0$$\n","where $\\delta$ is the gradient (called `grad` in the following code; see the lecture material for its derivation), $ϵ$ is the learning rate, and $\\textbf{x}_0$ is our first input sample in variable `X0`.\n","\n","Recall that $y$ is the desired output, i.e. `y[0]` in this Python code, and $\\hat{y}$ is called `y_out` here."]},{"cell_type":"code","metadata":{"id":"bs0EG3n9ve6J"},"source":["# the learning rate determines the step size in the gradient descent, you can experiment with different values if you want\n","learning_rate = 0.5\n","\n","# compute the gradient term\n","grad = (y_out - y[0]) * y_out * (1 - y_out)\n","\n","# Calculate the weight update\n","W_delta = - learning_rate * grad * X0\n","\n","print(W_delta)\n","\n","# Update the weights\n","W += W_delta"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ha1gJ0n9v2oV"},"source":["Let's try a forward propagation again with the same input."]},{"cell_type":"code","metadata":{"id":"fh40oHl2v6rs"},"source":["y_out=f(np.dot(W.T, X0))\n","\n","print('y_out=', y_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHc3os_AL54y"},"source":["You should notice that the result has moved (slightly!) towards the correct answer, that is zero. In order to converge to the right value we have to perform more iterations!"]},{"cell_type":"markdown","metadata":{"id":"8INLy50fyORN"},"source":["### Training iterations [TO COMPLETE]\n","\n","Let's define a complete training procedure for our model. In each iteration we have to perform the forward propagation, then we'll check how much the output differs from the target and propagate the error back (backward propagation).  We'll do this for each sample data point and then iterate this over and over again using a for loop."]},{"cell_type":"code","metadata":{"id":"PPXJwQm4ycgH"},"source":["# For the training we need to iterate over the dataset several times\n","num_iters = 1000\n","\n","# We'll also store the mean square error (MSE) in every round so we can see how it evolves\n","# mse is just an array to store these values at each round:\n","mse = np.zeros(num_iters)\n","\n","# Looping for the iterations\n","for it in range(num_iters):\n","    \n","    # For-loop going over each sample in X\n","    for n in range(len(X)):\n","        # Extract the n_th sample and the corresponding desired output\n","        x_n = np.reshape(X[n], (3,1))\n","        # Get the correponding target value\n","        y_target = y[n]\n","        \n","        # Forward propagation of the n_th sample\n","        y_out = f(np.dot(W.T, x_n))\n","\n","        # Let's keep track of the sum of squared errors\n","        mse[it] += # [TO COMPLETE] compute squared error between y_target and y_out\n","    \n","        # compute the gradient\n","        grad = (y_out - y_target) * y_out * (1 - y_out) \n","    \n","        # Calculate the weights update\n","        W_delta = - learning_rate * grad * x_n\n","\n","        # Update the weights\n","        W += W_delta\n","\n","    # Divide by the number of elements to get the mean of the squared errors\n","    mse[it] /= len(X)\n","\n","# Now let's see the output for each input sample with the trained weights\n","# Using batch mode (see next section) we can do this in a single line\n","print(\"Output after training, y_out\")\n","y_out = f(np.dot(X, W))\n","print(y_out)\n","print(\"Target output, y\")\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yT1qLBvBzpDc"},"source":["After the training phase, the output of the network is fairly close to the target output.\n","Notice how for the fourth input sample, the zero vector, this training procedure without the bias term does not update the corresponding weights and always gives $$\\hat{y}_4 = f(\\textbf{W}^T \\textbf{x}_4)= f(0) = \\frac{1}{2} $$\n","\n","How many iterations were required in order to obtain this result? We have set the number of the iteration to $1000$, but it is interesting to investigate the trend of the error trought the training. In the next homework, we will discuss how to select the right number of iterations (also known as *epochs*), for now let's just plot its behaviour:"]},{"cell_type":"code","metadata":{"id":"a6RsKlA7zrw3"},"source":["plt.figure()\n","plt.plot(range(num_iters), mse, label=\"MSE\")\n","l = plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiS6hr-eMWR1"},"source":["You should see the error going down pretty quickly at the beginning and then slowing down."]},{"cell_type":"markdown","metadata":{"id":"Zoz4y02rxorc"},"source":["### Batch training\n","\n","With real-world data it is unefficient to handle each example one-by-one like we did above.  Instead, one typically uses a set, so called *mini-batch*, of several input examples at once.\n","\n","Let's consider a subset $\\tilde{\\textbf{X}} ⊆ \\textbf{X}$ of samples from the training set. Each of these samples is one row in $\\tilde{\\textbf{X}}$, instead of a single column vector as before. The forward propagation step looks a bit different mathematically: $\\hat{\\textbf{y}} = f(\\tilde{\\textbf{X}}\\textbf{W})$.\n","\n","Our whole dataset can be forward propagated without a for loop:"]},{"cell_type":"code","metadata":{"id":"pwc6hfIlxspG"},"source":["y_out = f(np.dot(X, W))\n","print(y_out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H8pBN8voyLGv"},"source":["so we will get the corresponding output (each value in `y_out`) for each input (each row in `X`) in single matrix multiplication.  The error and weight updates can all be calculated in a single go, using matrix multiplications similarly to the steps we did above with single vectors.\n","\n","However, in these exercises we'll stick to looping over one sample at a time, as the batch training mode makes it a bit more complicated to understand and does not lead to any significant speed advantage."]},{"cell_type":"markdown","metadata":{"id":"7wv8Vplo1F7E"},"source":["## Exercise 1.2: The XOR problem"]},{"cell_type":"markdown","metadata":{"id":"oiI7bIaY1OVI"},"source":["Now let's try a slightly more difficult example. We'll use the same input data, but a different desired output. \n","\n","||Input|     |    Output|\n","|:----:|:---:|:---:|---:|\n","| 1    | 1   | 0   | 0  |\n","| 1    | 0   | 0   | 1  |\n","| 0    | 1   | 0   | 1  |\n","| 0    | 0   | 0   | 0  |\n","\n","In particular, the new input-output configuration represents the XOR problem (between the first two components). This problem is interesting because it can not be solved by using a single layer perceptron. Indeed, you will need (at least) a two-layer network to solve it.\n","In this exercise we will first show that the network that we defined in the previous exercise can not solve the XOR problem,  then we will define a two-layer perceptron able to compute the correct solution."]},{"cell_type":"code","metadata":{"id":"MuSfKOpD1TqJ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt \n","\n","np.random.seed([42])\n","\n","X = np.array([[1,1,0],\n","              [1,0,0],\n","              [0,1,0],\n","              [0,0,0]])\n","    \n","y = np.array([[0,1,1,0]]).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FphvrH-1e5O"},"source":["As we did in the previous exercise, let's initialize the weights and define the activation function (also in this exercise we will use the sigmoid function)."]},{"cell_type":"code","metadata":{"id":"aMeTD8cb1kNm"},"source":["# Weights initialization\n","W = 2 * np.random.random((3,1)) - 1\n","\n","# Activation function\n","def f(x):\n","    return 1 / (1+np.exp(-x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unbPv9pn3kVE"},"source":["Now let's run the network defined in Exercise 1.1 to check whether it is able to solve the XOR problem."]},{"cell_type":"code","metadata":{"id":"PGlUPnvi31fL"},"source":["num_iters = 1000\n","learning_rate = 0.5\n","\n","for it in range(num_iters):\n","    for n in range(len(X)):\n","        x_n = np.reshape(X[n], (3,1))\n","        y_target = y[n]\n","        \n","        # Forward propagation\n","        y_out = f(np.dot(W.T, x_n))\n","\n","        # Compute the Gradient\n","        grad = (y_out - y_target)*y_out*(1 - y_out)\n","    \n","        # Calculate the weights update\n","        W_delta = -learning_rate * grad * x_n\n","\n","        # Update the weights\n","        W += W_delta\n","\n","\n","# Now let's see the output for each input sample with the trained weights\n","# Using batch mode we can do this in a single line\n","y_out = f(np.dot(X, W))\n","print(\"Output after training, y_out\")\n","print(y_out)\n","print(\"Desired output, y\")\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myYbripS39R-"},"source":["### Two layers network [TO COMPLETE]\n","As you can see the network is not able to solve the problem, it's not even close! Indeed, it's just predicting $0.5$ for all the inputs. You could try to increase the number of iterations but it won't help either.\n","Let's add a single hidden layer, for example with 4 hidden nodes (you can try out and change this number as well).\n","The input to the network is a vector $\\mathbf{x}$ as before.  The first hidden layer calculates $\\textbf{h} = f(\\textbf{W}_1^T\\mathbf{x})$ (note that now $\\textbf{W}_1 \\in \\mathbb{R}^{3 \\times 4}$).  The output layer computes $\\hat{y} = f(\\textbf{W}_2^T\\textbf{h})$. Remember that $\\hat{y}$ is called `y_out` in th code, while ${W}_2 \\in \\mathbb{R}^{4 \\times 1}$.\n","\n","As usual, we'll start by initializing the weights randomly:"]},{"cell_type":"code","metadata":{"id":"Llke92kx4MRR"},"source":["num_hidden = 4\n","\n","# initialize weights randomly with zero mean and uniformly distributed values in [-1,1]\n","W_1 = 2 * np.random.random((3,num_hidden)) - 1\n","W_2 = 2 * np.random.random((num_hidden,1)) - 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnQfHhXV7dO0"},"source":["We have to define the training procedure in order to train the two-layers neural network:"]},{"cell_type":"code","metadata":{"id":"slOBh8IJ4tSK"},"source":["num_iters = 2000\n","learning_rate = 0.5\n","\n","mse = np.zeros(num_iters)\n","\n","for it in range(num_iters):\n","    for n in range(len(X)):\n","        x_n = np.reshape(X[n], (3,1))\n","        y_target = y[n]        \n","        \n","        # Forward propagation\n","        h = # [TO COMPLETE] Calculate h\n","    \n","        y_out = # [TO COMPLETE] Calculate y_out\n","        \n","        # Let's keep track of the sum of squared errors\n","        mse[it] += #[TO COMPLETE]: compute squared error between y_target and y_out\n","\n","        # [TO COMPLETE] Compute the gradient       \n","        # hint: you can do this by performing a for loop over i (hidden nodes) and k (input nodes) and calculate \n","        # each W_1_ik update separately\n","       \n","        # [TO COMPLETE] Update the weights\n","        # Note: it's important the W weights are updated at the end,\n","        # the above calculation should be done with the old weights\n","        \n","\n","    # Divide by the number of elements to get the mean of the squared errors\n","    mse[it] /= len(X)\n","\n","y_out = f(np.dot(f(np.dot(X, W_1)), W_2))\n","print(\"Output after training, y_out\")\n","print(y_out)\n","print(\"Target output, y\")\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1nYoyoR2fANg"},"source":["Now you should see outputs very similar to the desired ones!\n","Finaly, let's plot again the MSE behaviour:"]},{"cell_type":"code","metadata":{"id":"YKuG4wee9kIc"},"source":["plt.figure()\n","plt.plot(range(num_iters), mse, label=\"Two-layer NN MSE\")\n","l = plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAmf-t0PurMP"},"source":["## Exercise 1.3: Handwritten digits classification\n","In this exercise, we try to apply what we learned in the previous exercise in a real-world scenario. In particular, we consider a simple digits classification problem. The model turns out to be similar to the perceptron implemented in Exercise 1.1, but here we will use softmax activation function and cross-entropy loss function. The idea is to create a model that has in input an image of a handwritten digit and that return a vector of 10 probabilities (one for each possible digit 0 - 9). "]},{"cell_type":"markdown","metadata":{"id":"IAYb_cd8vkG-"},"source":["### Dataset\n","The dataset that we will use in this exercise is included in [scikit-learn](https://scikit-learn.org/stable/), one of the major Machine Learning libraries. The dataset is called `load_digits` and contains several hundreds of samples. Each datapoint is made of the handwritten digit image (or rather its 8x8 pixel representation), that will be the input of our model, and the target digit value. \n","\n","Let's start by plotting one of this handwritten digit:"]},{"cell_type":"code","metadata":{"id":"MGrkAeWtufMk"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import load_digits\n","\n","np.random.seed([42])\n","\n","digits = load_digits()\n","\n","sample_index = 42\n","plt.figure(figsize=(4, 4))\n","plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n","           interpolation='nearest')\n","plt.title(\"Image true label: %d\" % digits.target[sample_index])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0OsnNJbv69J"},"source":["It is better to check how an input in the dataset $\\mathbf{X}$ and its related target $\\mathbf{y}$ are represented in the dataset:"]},{"cell_type":"code","metadata":{"id":"ei9Uda4av5q2"},"source":["data = np.asarray(digits.data, dtype='float32')\n","target = np.asarray(digits.target, dtype='int32')\n","\n","print(\"X:\",data)\n","print(\"y:\",target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So essentialy the dataset is a matrix with the (color) values for each pixel and for each image, whereas the target is the digit itself."],"metadata":{"id":"T9yg07qT3-Kr"}},{"cell_type":"markdown","metadata":{"id":"8Hc97pziwLKa"},"source":["#### One-hot encoding\n","In order to have a representation of the target that will be similar to the output of the model (i.e. $\\hat{y}=0$ or $1$), we will use one-hot encoding. Basically, the one-hot encoding allow us to encode a categorical integer feature using a one-of-K scheme, where each class is translated to a specific index of an array."]},{"cell_type":"code","metadata":{"id":"eF6jEnOmwRUk"},"source":["def one_hot(n_classes, y):\n","    return np.eye(n_classes)[y]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For example, if there are 10 classes in total and a sample belongs to class number 3, we can translate the output to the following lenght-ten array of 0 and 1 (class 3 is in the fourth index because we start counting from zero!):"],"metadata":{"id":"onD0qqMx6shM"}},{"cell_type":"code","metadata":{"id":"r4CISmJ39lts"},"source":["one_hot(n_classes=10, y=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkSruP-bwkki"},"source":["### Activation Function: Softmax [TO COMPLETE]\n","As activation function we will use the Softmax function: this particular function is very useful when we have to deal with multiclassification tasks and one-hot target because it turns numbers, a.k.a. logits (pre-activations), into $m$ probabilities that sum to one. Basically, Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.\n","$$\n","softmax(\\mathbf{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^{m}{e^{x_i}}}\n","$$\n","In our case we have in input a matrix $\\mathbf{X}$ where each row is a vector $\\mathbf{x}$, therefore the softmax function that we have to implements will be mathematically defined as:\n","$$\n","softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{m}{e^{x_i}}}\n","\\cdot\n","\\begin{bmatrix}\n","  e^{x_1}\\\\\\\\\n","  e^{x_2}\\\\\\\\\n","  \\vdots\\\\\\\\\n","  e^{x_m}\n","\\end{bmatrix}\n","$$\n"]},{"cell_type":"code","metadata":{"id":"9BuHFgAKws_v"},"source":["def softmax(X):\n","    #[TO COMPLETE] define the softmax function"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KP7xOmNqsE-R"},"source":["### Loss Function: Cross Entropy ###\n","Usually, a neural network classifier that use the softmax function in the final layer is trained using Cross-Entropy as loss function:\n","$$H(P,Q)=-E_{x \\sim P}[log \\;Q(x)]$$"]},{"cell_type":"code","metadata":{"id":"UYnPJ1lWsFkP"},"source":["EPSILON = 1e-8 # this is needed for numerical stability\n","\n","def cross_entropy(Y_true, Y_pred):\n","    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred) # make sure the dimensions are right\n","    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n","    return -np.mean(loglikelihoods)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R2jR9ijztQWP"},"source":["### Weights Initailiazation\n","\n","Similarly to what we did in previous exercises, we have to initialize the weights but in this case we will consider the bias term as well. Therefore we define the weights $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$ and the bias $\\mathbf{b}\\in\\mathbb{R}^m$, where $n$ is the input size and $m$ is the number of classes.\n","Now we can define the output of our model as\n","\n","$$\\hat{\\mathbf{y}}=softmax(\\textbf{W} \\textbf{x}+\\mathbf{b})$$\n","\n"]},{"cell_type":"code","metadata":{"id":"FtgOtaVL-4xd"},"source":["np.random.seed([42])\n","\n","input_size = data.shape[1]\n","n_classes = len(np.unique(target))\n","\n","W = np.random.uniform(size=(input_size,n_classes), high=0.1, low=-0.1)\n","b = np.random.uniform(size=n_classes, high=0.1, low=-0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Ab3_mFz-9kG"},"source":["Let's consider a sample from the training set, and plot the current (and incorrect) output prediction probability of our model before training it."]},{"cell_type":"code","metadata":{"id":"6VpIr1KY_CTd"},"source":["y_out = softmax(np.dot(data[sample_index], W) + b)\n","\n","plt.bar(range(n_classes), y_out, label=\"Prediction\", color=\"red\")\n","plt.ylim(0,1,0.1)\n","plt.xticks(range(n_classes))\n","plt.legend()\n","plt.ylabel(\"Probability\")\n","plt.title(\"Target: \" + str(target[sample_index]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r4DYTEEh_QDf"},"source":["### Training procedure [TO COMPLETE]:\n","As in the previous exercise let's define a training procedure. Note that in this case, we have to compute the gradient according to the softmax function and the loss function that the training has to optimize. \n","\n","Hence, the gradient for the weights $\\textbf{W}$ is:\n","\n","$\\nabla_W=(\\mathbf{\\hat{y}}-\\mathbf{y}) \\cdot \\mathbf{x}$\n","\n","while for the bias is:\n","\n","$\\nabla_b=(\\mathbf{\\hat{y}}-\\mathbf{y})$\n","\n","During the training procedure let's also compute the accuracy of the predictions and the loss value at each iteration:\n","\n"]},{"cell_type":"code","metadata":{"id":"OcQfByoE_a6v"},"source":["num_iters = 50\n","learning_rate = 0.0005\n","\n","for it in range(num_iters):\n","    iteration_accuracy = []\n","    iteration_loss = []\n","    for i, (X, y) in enumerate(zip(data, target)):\n","\n","        # Forward propagation\n","        y_out = # [TO COMPLETE] compute forward propagation using the softmax \n","                # function previously defined as activation function\n","\n","        #Evaluate the output error\n","        pred_err = y_out - one_hot(n_classes, y)\n","\n","\n","        # [TO COMPLETE] Compute the gradient (for the weights and the bias)\n","\n","        # [TO COMPLETE] Update the weights and the bias\n","        # Note: it's important the weights W and the bias b are updated at the end\n","        # the above calculation should be done with the old weights\n","\n","        iteration_accuracy.append(np.argmax(y_out) == y)\n","        iteration_loss.append(cross_entropy(one_hot(n_classes,y), y_out))\n","\n","    print(\"iteration: \", it, \" -- accuracy: \", np.mean(np.asarray(iteration_accuracy)), \" -- loss: \", np.mean(iteration_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnJH8lWaxtc4"},"source":["As you can see during the training the accuracy increases after each iteration, while the loss function value progressively declines.\n","\n","Finally, let's check how the prediction capability of our model changes after the training:"]},{"cell_type":"code","metadata":{"id":"jGrzV9w6_lwz"},"source":["y_pred = softmax(np.dot(data[sample_index], W) + b)\n","plt.bar(range(n_classes), y_pred, label=\"Prediction\", color=\"red\")\n","plt.ylim(0, 1, 0.1)\n","plt.xticks(range(n_classes))\n","plt.legend()\n","plt.ylabel(\"Probability\")\n","plt.title(\"Target: \" + str(target[sample_index]))\n","plt.show()"],"execution_count":null,"outputs":[]}]}